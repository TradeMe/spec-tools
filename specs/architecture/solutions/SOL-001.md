# SOL-001: Rate Limiting Solution Architecture

**Version**: 1.0
**Created**: 2025-10-31
**Status**: Active

## Overview

This document describes the solution architecture for implementing distributed rate limiting across the API gateway infrastructure, addressing [REQ-006](../../requirements/REQ-006.md).

The architecture employs a distributed, multi-tier approach with Redis as the backing store for quota state, a gateway-level rate limiting middleware for decision enforcement, and a management service for policy configuration. The design prioritizes low latency, high availability, and operational simplicity while supporting complex rate limiting scenarios.

**Key Design Principles**:
- **Low Latency**: Rate limit decisions in <5ms to minimize request overhead
- **High Availability**: No single point of failure; graceful degradation
- **Horizontal Scalability**: Linear scaling with additional instances
- **Operational Simplicity**: Declarative configuration; minimal maintenance
- **Observability**: Comprehensive metrics and logging for debugging

## Addresses

- [REQ-006](../../requirements/REQ-006.md): Distributed Rate Limiting System

## System Context

The rate limiting system operates within the broader API management platform:

```
┌─────────────────┐
│   API Clients   │
│ (Mobile, Web,   │
│  Partners, etc) │
└────────┬────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│         Load Balancer (L7)              │
└────────┬────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│      API Gateway Cluster (N nodes)      │◄──── This Solution
│  ┌────────────────────────────────┐     │
│  │  Rate Limiting Middleware      │     │
│  └────────────────────────────────┘     │
└────────┬────────────────────────────────┘
         │
         ├──────────┐
         │          │
         ▼          ▼
┌────────────┐   ┌──────────────────┐
│  Backend   │   │  Redis Cluster   │◄──── This Solution
│  Services  │   │ (Rate Limit Store)│
└────────────┘   └──────────────────┘
                          ▲
                          │
                 ┌────────┴────────┐
                 │  Rate Limit     │◄──── This Solution
                 │  Management API │
                 └─────────────────┘
```

**External Dependencies**:
- **Load Balancer**: Routes traffic to gateway instances; provides SSL termination
- **Backend Services**: Actual API implementation protected by rate limiting
- **Monitoring System**: Collects metrics from rate limiter (Prometheus/Datadog)
- **Configuration Store**: Stores rate limit policies (etcd or database)

**Interactions**:
- Gateway fetches/updates quota state from Redis for each request
- Management API updates rate limit policies in configuration store
- Gateways watch configuration store for policy updates
- All components emit metrics to monitoring system

## Components

### COMP-01: Rate Limiting Middleware

The Rate Limiting Middleware is a gateway plugin that intercepts incoming requests, evaluates them against configured policies, and enforces rate limits before forwarding to backend services.

**Responsibilities**:
- Extract client identifier from request (API key, IP, user ID, etc.)
- Retrieve current quota state from Redis
- Apply rate limiting algorithm (token bucket, sliding window, etc.)
- Update quota state in Redis
- Add rate limit headers to response
- Return HTTP 429 if rate limit exceeded
- Emit metrics for monitoring

**Technology**: Go (for performance) or Lua (for gateway integration)

**Key Algorithms**:
- Token bucket with refill
- Sliding window log
- Fixed window counter
- Concurrent request tracking

**Performance Targets**:
- <5ms p95 latency for rate limit decision
- Support 100,000 requests/sec per gateway instance
- <10MB memory overhead per gateway instance

### COMP-02: Redis Quota Store

Redis cluster provides distributed, high-performance storage for quota state across all gateway instances.

**Responsibilities**:
- Store quota counters with TTL
- Atomic increment operations for quota updates
- Support for pipelining batch operations
- Persistence for quota recovery after failures
- Replication for high availability

**Technology**: Redis 7.x with clustering enabled

**Data Structures**:
- **Sorted Sets**: For sliding window log (stores request timestamps)
- **Strings with TTL**: For simple counters (fixed window)
- **Hash Maps**: For multi-tier quotas (different limits per tier)
- **Lists**: For concurrent request tracking

**Scaling**:
- Horizontal scaling via Redis cluster sharding
- Read replicas for read-heavy workloads
- AOF persistence for durability
- 3-node minimum for quorum

### COMP-03: Rate Limit Management API

RESTful API service for managing rate limit policies, viewing quota usage, and testing policies before deployment.

**Responsibilities**:
- CRUD operations for rate limit policies
- Real-time quota usage queries
- Policy validation and testing
- Audit logging for policy changes
- Webhook notifications for limit violations

**Technology**: Python (FastAPI) or Go (Gin)

**Key Endpoints**:
- `POST /api/v1/policies` - Create rate limit policy
- `GET /api/v1/policies` - List all policies
- `PUT /api/v1/policies/{id}` - Update policy
- `DELETE /api/v1/policies/{id}` - Delete policy
- `GET /api/v1/quotas/{client_id}` - View client quota usage
- `POST /api/v1/quotas/{client_id}/reset` - Reset client quota
- `POST /api/v1/policies/{id}/test` - Test policy with sample traffic

**Authentication**: JWT-based auth with admin role required

### COMP-04: Policy Configuration Store

Distributed configuration store for rate limit policies, watched by gateway instances for real-time updates.

**Responsibilities**:
- Store rate limit policies with versioning
- Provide watch API for change notifications
- Ensure consistency across all gateways
- Support rollback to previous policy versions
- Handle concurrent policy updates

**Technology**: etcd or PostgreSQL with LISTEN/NOTIFY

**Schema**:
```json
{
  "policy_id": "uuid",
  "name": "Free Tier Hourly Limit",
  "client_tier": "free",
  "algorithm": "sliding_window",
  "limits": [
    {"period": "hour", "quota": 1000},
    {"period": "day", "quota": 10000}
  ],
  "endpoints": ["/api/*"],
  "created_at": "timestamp",
  "updated_at": "timestamp",
  "version": 3
}
```

### COMP-05: Metrics and Monitoring Component

Collects, aggregates, and visualizes rate limiting metrics for operational visibility and capacity planning.

**Responsibilities**:
- Collect metrics from all gateway instances
- Aggregate by client, tier, endpoint
- Alert on anomalous patterns
- Dashboard for real-time quota usage
- Historical analysis for trend detection

**Technology**: Prometheus (metrics) + Grafana (dashboards)

**Key Metrics**:
- `rate_limit_requests_total{tier, endpoint, action}` - Counter
- `rate_limit_decision_latency_seconds{quantile}` - Histogram
- `rate_limit_quota_used{client_id, tier}` - Gauge
- `rate_limit_redis_errors_total{operation}` - Counter
- `rate_limit_policy_updates_total` - Counter

**Alerting Rules**:
- High rate limit rejection rate (>10% for extended period)
- Redis latency exceeds threshold (>10ms p95)
- Gateway cannot reach Redis (fail-open engaged)
- Suspicious traffic patterns (sudden spike from single client)

## Interactions

### Request Flow (Happy Path)

1. **Client Request Arrives**:
   - Load balancer routes to gateway instance
   - Request enters gateway middleware stack

2. **Rate Limit Check**:
   - Middleware extracts client identifier (API key from header)
   - Middleware queries Redis for current quota state
   - Redis returns counter and TTL

3. **Decision Enforcement**:
   - Middleware applies token bucket algorithm
   - If quota available: decrement counter, allow request
   - If quota exhausted: return HTTP 429

4. **Response Headers Added**:
   - `X-RateLimit-Limit: 1000`
   - `X-RateLimit-Remaining: 847`
   - `X-RateLimit-Reset: 1698765600`

5. **Request Forwarded** (if allowed):
   - Gateway forwards to backend service
   - Backend processes and returns response
   - Gateway returns response to client

### Policy Update Flow

1. **Admin Updates Policy**:
   - Admin calls Management API: `PUT /api/v1/policies/123`
   - Management API validates policy schema
   - Management API writes to Configuration Store

2. **Configuration Propagation**:
   - Configuration Store triggers change event
   - All gateway instances watching configuration receive update
   - Gateways update in-memory policy cache

3. **New Policy Active**:
   - Within 10 seconds, all gateways enforce new limits
   - Metrics show policy version in use
   - Audit log records change with admin identity

### Failure Scenarios

**Redis Unavailable**:
- Gateway detects Redis connection failure
- Gateway switches to fail-open mode (configurable)
- Local in-memory quota tracking activated (best effort)
- Alert fires to operations team
- When Redis recovers, distributed mode resumes

**Policy Configuration Error**:
- Management API validates policy before saving
- Invalid policies rejected with error message
- Test endpoint allows validation before deployment
- Gateway instances ignore invalid policy updates

## Data Flow

### Quota State Flow

```
Request → Extract Client ID → Generate Redis Key
                ↓
        Query Redis: GET key
                ↓
        ┌───────────────┐
        │ State Exists? │
        └───────┬───────┘
                │
       ┌────────┴────────┐
       │                 │
     YES                NO
       │                 │
       ▼                 ▼
  Check Quota      Initialize Quota
       │                 │
       └────────┬────────┘
                │
        ┌───────▼────────┐
        │ Quota Exceeded?│
        └───────┬────────┘
                │
       ┌────────┴────────┐
       │                 │
      YES                NO
       │                 │
       ▼                 ▼
   Return 429      Decrement Quota
                          │
                          ▼
                   Update Redis: DECR key
                          │
                          ▼
                   Forward Request
```

### Multi-Tier Quota Check

For a single request, multiple quota checks may apply:

1. **Global System Quota**: 1M requests/minute (prevent DDoS)
2. **Per-Tier Quota**: 100K requests/hour (free tier limit)
3. **Per-Client Quota**: 1K requests/hour (individual client)
4. **Per-Endpoint Quota**: 10 requests/minute (expensive endpoint)

All checks must pass for request to proceed. First failure short-circuits.

## Technology Stack

**Gateway Layer**:
- **Platform**: NGINX with custom Lua module or Envoy with WASM plugin
- **Language**: Lua (NGINX) or Rust (Envoy WASM)
- **Deployment**: Kubernetes with horizontal pod autoscaling

**Storage Layer**:
- **Primary**: Redis 7.x cluster (3-5 nodes)
- **Configuration**: etcd 3.x cluster (3 nodes)
- **Monitoring**: Redis Exporter for Prometheus

**Management Layer**:
- **Framework**: FastAPI (Python 3.11+) or Gin (Go 1.21+)
- **Database**: PostgreSQL 15 for audit logs and configuration
- **Authentication**: OAuth 2.0 / JWT

**Observability**:
- **Metrics**: Prometheus
- **Dashboards**: Grafana
- **Logging**: Structured JSON logs to stdout → Loki
- **Tracing**: OpenTelemetry → Jaeger

**Infrastructure**:
- **Container**: Docker
- **Orchestration**: Kubernetes 1.28+
- **Cloud**: AWS (ElastiCache for Redis, EKS for K8s)

## Quality Attributes

### QA-01: Low Latency

**Requirement**: p95 latency <5ms for rate limit decision

**Approach**:
- Use Redis pipelining to batch operations
- Maintain local cache for policy configuration
- Optimize Lua scripts to minimize Redis roundtrips
- Use connection pooling to reduce overhead

**Verification**: Load testing with 100K req/s, measure latency

### QA-02: High Throughput

**Requirement**: Support 100,000 requests/second per gateway instance

**Approach**:
- Asynchronous Redis operations where possible
- Minimize memory allocations in hot path
- Use efficient data structures (avoid deep copies)
- Profile and optimize critical code paths

**Verification**: Benchmark testing with varying loads

### QA-03: Horizontal Scalability

**Requirement**: Linear performance scaling with additional instances

**Approach**:
- Stateless gateway design (all state in Redis)
- Consistent hashing for Redis key distribution
- No inter-gateway communication required
- Independent policy cache per instance

**Verification**: Test with 1, 5, 10, 20 gateway instances

### QA-04: High Availability

**Requirement**: 99.99% availability (52 minutes downtime/year)

**Approach**:
- Redis cluster with automatic failover
- Gateway fail-open mode when Redis unavailable
- Multiple availability zones for all components
- Health checks and automated recovery

**Verification**: Chaos testing, fault injection, failure scenarios

### QA-05: Operational Simplicity

**Requirement**: Minimal operational overhead for common tasks

**Approach**:
- Declarative policy configuration (no code changes)
- Automated deployment via GitOps
- Self-healing infrastructure (K8s)
- Comprehensive runbooks for common scenarios

**Verification**: Time common operational tasks (should be <5 minutes)

## Security Considerations

**Authentication and Authorization**:
- Management API requires admin authentication
- Rate limit bypass requires special permissions
- Audit logs for all policy changes
- Principle of least privilege for service accounts

**Data Protection**:
- Redis communication encrypted with TLS
- Quota data not considered sensitive (no PII)
- Configuration backups encrypted at rest
- Secure random generation for API keys

**Attack Prevention**:
- Rate limiter itself protected by rate limits
- Input validation on all API endpoints
- Protection against header injection attacks
- Redis ACLs to limit command access

**Audit and Compliance**:
- All policy changes logged with timestamp and actor
- Retention policy for audit logs (1 year minimum)
- Monitoring for unauthorized access attempts
- Regular security audits of configuration

## Deployment

**Deployment Model**: Blue-green deployment with canary testing

**Deployment Steps**:
1. Deploy new gateway version to 10% of instances (canary)
2. Monitor error rates and latency for 15 minutes
3. If metrics normal, deploy to remaining 90%
4. If issues detected, automatic rollback to previous version

**Configuration Changes**:
- Policy updates via Management API (no deployment needed)
- Changes propagate to all instances within 10 seconds
- Rollback capability via version history

**Scaling Operations**:
- Gateway horizontal pod autoscaling based on CPU (target: 70%)
- Redis scaling via cluster node addition (manual)
- Management API scales with gateway (same ratio)

**Disaster Recovery**:
- Redis AOF backup every 1 hour to S3
- Configuration backup to git repository
- RTO: 15 minutes, RPO: 1 hour
- Automated recovery procedures documented

## Future Enhancements

**Phase 2 (Q2 2026)**:
- Machine learning based anomaly detection
- Automatic limit adjustment based on traffic patterns
- Geographic rate limiting (different limits per region)
- Cost-based rate limiting (charge based on complexity)

**Phase 3 (Q3 2026)**:
- Predictive scaling based on historical patterns
- Advanced traffic shaping and prioritization
- Integration with billing system for usage-based pricing
- GraphQL query complexity based rate limiting
